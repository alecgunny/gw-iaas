{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22a8f06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bringing AI to Gravitational Wave Physics\n",
    "## Alec Gunny, Dylan Rankin, Jeffrey Krupa, Muhammed Saleem, Tri Nguyen, Michael Coughlin, Philip Harris, Erik Katsavounidis, Steven Timm, Burt Holzman\n",
    "\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/institute-logos.png\" height=\"auto\" width=\"600px\" />\n",
    "</div>\n",
    "\n",
    "<div class=\"padded\">\n",
    "    <div class=\"padded\">\n",
    "        <sub><sup><a href=\"https://arxiv.org/abs/2108.12430\">https://arxiv.org/abs/2108.12430</a></sup></sub>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090a7fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "- Introduction\n",
    "- A naive pipeline\n",
    "- Inference-as-a-Service\n",
    "- LIGO use cases\n",
    "- Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0d9ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "## Deep Learning in GW Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e74c2d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-Messenger Astrophysics (MMA)\n",
    "Use signals detected in gravitational strain to alert observers of other messengers for followup\n",
    "\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/mma.png\" height=\"auto\" width=\"1000px\" />\n",
    "</div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0c5c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LIGO trigger system\n",
    "<img src=\"images/trigger.png\" height=\"auto\" width=\"600px\" class=\"left\"/>\n",
    "<img src=\"https://research.cs.wisc.edu/htcondor/manual/v7.6/img7.png\" height=\"auto\" width=\"400px\" class=\"right\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb21a11",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LIGO use cases - DeepClean\n",
    "- Noise regression in the gravitational wave detector data from instrumental artifacts and environmental contamination.\n",
    "- Uses data from on-site sensors monitoring the instrumental and environmental signals (witness channels) \n",
    "- Makes noise predictions using the couplings between witness channels and  GW data. \n",
    "- Generic enough to subtract linear, non-linear, and non-stationary coupling mechanisms.\n",
    "- Astrophysical signals remain unaffected by the subtraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ef8b7",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<figure>\n",
    "    <img src=\"images/deepclean-plots.png\" height=\"auto\" width=\"400px\" />\n",
    "    <figcaption>Ormiston, Rich, et al. “Noise Reduction in Gravitational-Wave Data via Deep Learning.” Physical Review Research, vol. 2, no. 3, July 2020, p. 033066. arXiv.org, doi:10.1103/PhysRevResearch.2.033066</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126df071",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LIGO use cases - BBHnet\n",
    "- Archetypal search for binary blackhole mergers using strain data from both detectors\n",
    "- Paper forthcoming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78331fd",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/bbh-plot.png\" height=\"auto\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c007f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Real-time inference with deep learning\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/dc-offline-no-legend.png\" height=\"auto\" width=\"800px\" />\n",
    "</div>\n",
    "\n",
    "- Massive reductions in processing time\n",
    "- Not necessarily trivial to achieve\n",
    "- In this talk, we'll outline a computing paradigm for achieving accelerated processing at scale with deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a4f9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The traditional deep learning inference paradigm\n",
    "### Building an example pipeline using PyTorch\n",
    "- Illustrate the challenges outlined above\n",
    "- Motivate the requirements of an improved paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a88406",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Begin with a few imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034de904",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Empty, Queue\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# some functionality will be necessarily\n",
    "# buried in here. If you're interested,\n",
    "# feel free to take a look\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248fefb3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define our model: a simple multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401af3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for size in hidden_sizes:\n",
    "            self.layers.append(torch.nn.Linear(input_size, size))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            input_size = size\n",
    "\n",
    "        self.layers.append(torch.nn.Linear(input_size, 1))\n",
    "        self.layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f64444",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "During training:\n",
    "- instantiate an instance of this model then optimize its parameters\n",
    "- export these optimized parameters for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803464d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 64\n",
    "HIDDEN_SIZES = [256, 128, 64]\n",
    "model = MLP(INPUT_SIZE, HIDDEN_SIZES)\n",
    "\n",
    "# typically do some training here\n",
    "# for i in range(num_epochs):\n",
    "#    for x in dataset:\n",
    "#        do_a_gradient_step(model, x)\n",
    "# now our model has optimized parameters\n",
    "\n",
    "# export these parameter values somewhere\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0478d2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At inference time, load in these optimized model weights and use them to map inputs to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb0597f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4779905], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the operations that constitute a model\n",
    "inference_model = MLP(INPUT_SIZE, HIDDEN_SIZES)\n",
    "\n",
    "# set the values of the parameters of these\n",
    "# operations to their optimized values\n",
    "inference_model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "# create an array of input data to process\n",
    "x = np.random.randn(INPUT_SIZE).astype(\"float32\")\n",
    "with torch.no_grad():\n",
    "    # use the model infer predictions on the input data\n",
    "    y = inference_model(torch.from_numpy(x))\n",
    "\n",
    "# do some downstream processing on the data\n",
    "y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339a192",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hetergeneous computing\n",
    "- Often faster to use **accelerated** hardware like GPUs or FPGAs for computing inference\n",
    "- Systems with multiple types of processors, e.g. CPUs and GPUs, referred to as **hetergeneous**\n",
    "\n",
    "<img src=\"images/cpu-inference.png\" height=\"auto\" width=\"500px\" class=\"left\"/>\n",
    "<img src=\"images/hetergeneous-inference.png\" height=\"auto\" width=\"500px\" class=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5fdbf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Heterogeneous inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44b0112",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47799045], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move the model to the GPU\n",
    "inference_model.cuda(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # move the data on to the GPU\n",
    "    x = torch.from_numpy(x).cuda(0)\n",
    "\n",
    "    # execute inference on the GPU\n",
    "    y = inference_model(x)\n",
    "\n",
    "# move these predictions back to the CPU\n",
    "# for downstream processing\n",
    "y.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea802ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inference on a dataset\n",
    "- Generally interested in using the model for many thousands or millions of inferences\n",
    "- Start with the simplest case: a dataset that can fit into memory at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae0caed",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 141 ms, total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "N = 5 * 10**5  # number of observations in our dataset\n",
    "dataset = np.random.randn(N, INPUT_SIZE).astype(\"float32\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def do_some_inference(model, dataset, batch_size=8, device_index=0):\n",
    "    # move the data to the GPU in bulk\n",
    "    gpu_dataset = torch.from_numpy(dataset).cuda(device_index)\n",
    "\n",
    "    # iterate through it in batches and yield predictions\n",
    "    dataset = torch.utils.data.TensorDataset(gpu_dataset)\n",
    "    for [x] in torch.utils.data.DataLoader(dataset, batch_size=batch_size):\n",
    "        y = model(x)\n",
    "        yield y.cpu().numpy()\n",
    "\n",
    "# run through the dataset and get a rough time estimate\n",
    "%time outputs = [y for y in do_some_inference(inference_model, dataset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ae916",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Seems to work pretty fast, but how well are we utilizing the GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4b12dd",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d283bc8c7bae4b94ace2f5e8ffdfe34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with utils.GpuUtilProgress(gpu_ids=0) as progbar:\n",
    "    task_id = progbar.add_task(\"[cyan]Inference\", total=N)\n",
    "\n",
    "    outputs = []\n",
    "    for y in do_some_inference(inference_model, dataset):\n",
    "        outputs.append(y)\n",
    "        progbar.update(task_id, advance=len(y))\n",
    "\n",
    "output = np.concatenate(outputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59efdb9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Yikes, only around 25%!\n",
    "- GPUs are expensive, can we improve things via parallel execution (assuming we can't change the batch size)?\n",
    "- First attempt: Naive (and sloppy) implementation using threading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc2bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue()\n",
    "\n",
    "def task(dataset_chunk, device_index):\n",
    "    # for each inference task, create a copy of the\n",
    "    # model on the indicated GPU device\n",
    "    model = MLP(INPUT_SIZE, HIDDEN_SIZES).cuda(device_index)\n",
    "    model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "    # iterate through our dataset and send\n",
    "    # the results back to the main thread\n",
    "    for y in do_some_inference(\n",
    "        model, dataset_chunk, device_index=device_index\n",
    "    ):\n",
    "        q.put(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209e056",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Run this task on multiple parallel threads (if we tried to use processes, Torch would complain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c68578c1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735a23b2b70948aaa577725425465716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_jobs = 4\n",
    "\n",
    "with utils.GpuUtilProgress(0) as progbar:\n",
    "    task_id = progbar.add_task(f\"Inference with {num_jobs} jobs\", total=N)\n",
    "\n",
    "    # create a pool of threads to do inference in parallel\n",
    "    with ThreadPoolExecutor(4) as pool:\n",
    "        # split the dataset into chunks and submit\n",
    "        # inference on them as tasks to the pool\n",
    "        [pool.submit(task, x, 0) for x in np.array_split(dataset, num_jobs)]\n",
    "\n",
    "        # iterate through the dataset\n",
    "        outputs = []\n",
    "        while len(outputs) < N:\n",
    "            outputs.extend(q.get())\n",
    "            progbar.update(task_id, completed=len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33512b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It looks like things actually got worse!\n",
    "- Extracting good GPU performance is rarely simple or intuitive\n",
    "- What next?\n",
    "\n",
    "After spending a few hours perusing the PyTorch documentation and forums and experimenting, we come up with the following basic functional implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c56f4d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def parallel_inference(X, num_gpus, jobs_per_gpu, progbar):\n",
    "    num_jobs = num_gpus * jobs_per_gpu\n",
    "    task_id = progbar.add_task(\n",
    "        f\"[cyan]{num_gpus} GPUs/{num_jobs} jobs\",\n",
    "        total=len(X),\n",
    "        start=False\n",
    "    )\n",
    "\n",
    "    # we need special queue and value objects\n",
    "    # specific to process spawning\n",
    "    smp = torch.multiprocessing.get_context(\"spawn\")\n",
    "    q = smp.Queue()\n",
    "    sync = smp.Value(\"d\", 0.0)\n",
    "\n",
    "    # pass a bunch of arguments into each\n",
    "    # process that we need to spawn\n",
    "    # note that we have to pass copies of some\n",
    "    # of our local functions that live in `utils`\n",
    "    # since we can't unpickle elsewhere functions\n",
    "    # which are defined in __main__\n",
    "    args = (\n",
    "        X,  # the full dataset\n",
    "        utils.MLP,  # the module class to use for inference\n",
    "        [INPUT_SIZE, HIDDEN_SIZES],  # arguments to initialize the module\n",
    "        utils.do_some_inference,  # the inference funcntion to use\n",
    "        q,  # the queue to put the results in\n",
    "        sync,  # a task synchronizer\n",
    "        jobs_per_gpu,\n",
    "        num_gpus\n",
    "    )\n",
    "\n",
    "    # spawn parallel jobs across all GPUs.\n",
    "    # We have to host the `parallel_inference_task` function\n",
    "    # in a separate module for the same pickling pickle\n",
    "    # described above\n",
    "    procs = torch.multiprocessing.spawn(\n",
    "        utils.parallel_inference_task,\n",
    "        args=args,\n",
    "        nprocs=num_jobs,\n",
    "        join=False\n",
    "    )\n",
    "\n",
    "    # wait to synchronize until all models load\n",
    "    # so that we can compare throughput better\n",
    "    while sync.value < num_jobs:\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    # increment the synchronizer one more\n",
    "    # time to kick off the jobs\n",
    "    sync.value += 1\n",
    "    progbar.start_task(task_id)\n",
    "\n",
    "    # collect all the (unordered) inputs\n",
    "    outputs = []\n",
    "    while len(outputs) < N:\n",
    "        try:\n",
    "            # try to get the next result in\n",
    "            # in the queue and increment everything\n",
    "            outputs.extend(q.get_nowait())\n",
    "            progbar.update(task_id, completed=len(outputs))\n",
    "        except Empty:\n",
    "            # if the queue is empty _and_ all the jobs\n",
    "            # are dead, we're done here\n",
    "            if procs.join(0.01):\n",
    "                break\n",
    "\n",
    "    # concatenate the outputs and return\n",
    "    return np.stack(outputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a0e7b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does GPU usage and time-to-completion scale with the number of jobs/GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950624d2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a309035fdcb4757a41be691b4a3ecb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with utils.GpuUtilProgress([0, 1]) as progbar:\n",
    "    y = parallel_inference(dataset, num_gpus=1, jobs_per_gpu=2, progbar=progbar)\n",
    "    y = parallel_inference(dataset, num_gpus=1, jobs_per_gpu=4, progbar=progbar)\n",
    "    y = parallel_inference(dataset, num_gpus=2, jobs_per_gpu=4, progbar=progbar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c12883",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Things seem to scale pretty well! But this is still far from ideal:\n",
    "- Framework specific\n",
    "    - No help if we want to extend to other frameworks\n",
    "    - Torch is pretty unique in having this functionality at all\n",
    "- The code is complicated and required a lot of non-physics expertise to build\n",
    "    - Non-trivial to reconstruct for new applications\n",
    "- Extremely contrived example, breaks down in most real use cases\n",
    "    - Explore a few cases to show how"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4066b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Throughput too low\n",
    "#### _The constraints of our use case demand that we further reduce processing time by an order of magnitude_\n",
    "- Not obvious how to simply extend this code to multi-node\n",
    "- Scaling not dynamic\n",
    "    - Have to pick a level of parallelism and hope the resources are available to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499bfd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Throughput too high\n",
    "####  _Data generation process is slow, needs to be parallelized to saturate GPU throughput_\n",
    "- Low GPU utilization with local resources. Not obvious how this code can:\n",
    "    - Scale to multiple clients\n",
    "    - Allow other users to leverage spare cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce940ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model ensembling\n",
    "#### _Connecting multiple models in a single pipeline_\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/model_sharing.png\" height=\"auto\" width=\"400px\" class=\"center\" />\n",
    "    <img src=\"images/model_ensemble.png\" height=\"auto\" width=\"400px\" class=\"center\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e5f0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model ensembling\n",
    "Naive implemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35fd21e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed5f23910c349d0be9d0cea7daba813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def do_some_multi_model_inference(noise_remover, model, dataset, batch_size=8, device_index=0):\n",
    "    gpu_dataset = torch.from_numpy(dataset).cuda(device_index)\n",
    "    dataset = torch.utils.data.TensorDataset(gpu_dataset)\n",
    "\n",
    "    for [x] in torch.utils.data.DataLoader(dataset, batch_size=batch_size):\n",
    "        cleaned = noise_remover(x)\n",
    "        y = model(cleaned)\n",
    "        yield x.cpu().numpy()\n",
    "\n",
    "noise_remover = utils.NoiseRemovalModel(INPUT_SIZE, [32, 16]).cuda(0)\n",
    "with utils.GpuUtilProgress(0) as progbar:\n",
    "    task_id = progbar.add_task(\"[cyan]Ensemble inference\", total=N)\n",
    "    for y in do_some_multi_model_inference(noise_remover, inference_model, dataset):\n",
    "        progbar.update(task_id, advance=len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ea907",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Ensembling\n",
    "\n",
    "Once again, it _works_, but scaling it up is non-trivial:\n",
    "- Models may require different levels of parallelism to keep any one model from bottlenecking the other (top on right)\n",
    "- Most efficient implementation would have models executing asynchronously, with tensors passed between GPUs (bottom on right)\n",
    "- If the models utilize different frameworks, this problem becomes exponentially harder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a350b",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div class=\"padded\">\n",
    "    <figure>\n",
    "        <img src=\"images/bottleneck-both.png\" height=\"auto\" width=\"800px\"/>\n",
    "        <figcaption>Model 1 throughput too high for model 2, need to run more concurrent instances of model 2 to maximize throughput</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"images/async.png\" height=\"auto\" width=\"800px\"/>\n",
    "        <figcaption>Model 2 computes inference on last output from Model 1, while Model 1 begins inference on next input</figcaption>\n",
    "    </figure>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c149e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distribution\n",
    "#### _Who do you want to use your model?_\n",
    "\n",
    "- How much expertise should someone need to have to utilize your model in their pipeline?\n",
    "- How much do they need to know about how your model is implemented?\n",
    "- How will they be kept up-to-date when you retrain the model or improve the architecture?\n",
    "    - Will these updates change their pipeline?\n",
    "- What if they don't have access to accelerators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c4cdf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Takeaways\n",
    "- Efficiently scheduling cross-platform, multi-GPU, multi-model asynchronous DL inference is hard\n",
    "    - Not going to get 3 orders of magnitude with this\n",
    "- Inference is just one piece of your pipeline. Really even just one line:\n",
    "```python\n",
    "y = model(x)\n",
    "```\n",
    "    How and where `model(x)` happens is largely irrelevant to everything else\n",
    "\n",
    "So:\n",
    "- Manage this piece separately to hide these details\n",
    "- Scale it to meet the rate at which you can generate `x`s or how quickly you need `y`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14159eb9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Not just inference problems\n",
    "- Large-scale offline validation required to quantify advantages of new research and automate model re-training\n",
    "    - Faster processing times mean faster iteration on novel ideas\n",
    "- Deployment on \"real\", uncontrolled data critical to:\n",
    "    - identifying failure modes and improving understanding of model behavior\n",
    "    - evaluating the correlation between validation metrics and true success criterion\n",
    "        - Identify and remove signal leakage in training pipelines\n",
    "        - Align training and test settings - can our algorithm optimize our latency/throughput/cost function better than its alternative?\n",
    "- O4 run will collect ~2 TB/day/detector\n",
    "    - Answering these questions now will ensure that we have the right combination of systems/algorithms in place to extract as much physics as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046926a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference-as-a-Service\n",
    "### An alternative paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199c22c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Learning Inference - Challenges\n",
    "<div class=\"float200 padded\">\n",
    "    <img src=\"images/hardware-logos.png\" width=\"300px\" height=\"auto\" class=\"right unpadded\"/>\n",
    "    <div class=\"left\">\n",
    "        <p>Access to and familiarity with accelerated hardware </p>\n",
    "        <p>(GPUs, TPUs, FPGAs)</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/framework-logos.png\" width=\"300px\" height=\"auto\" class=\"left padded\" />\n",
    "    <p class=\"right\"> Managing, leveraging, and translating across deep learning software stacks\n",
    "</div>\n",
    "\n",
    "<div></div>\n",
    "<div></div>\n",
    "<div class=\"float100\">\n",
    "    <img src=\"images/distributing-nns.png\" width=\"250px\" height=\"auto\" class=\"right unpadded\"/>\n",
    "    <p class=\"left\">Distributing updated models to dependent users and applications</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952356a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inference-as-a-Service\n",
    "The **inference-as-a-service** (Iaas) paradigm addresses these issues\n",
    "- Out-of-the-box software optimized for efficiently executing complex asynchronous workloads across devices\n",
    "    - Hardware _and_ framework agnostic\n",
    "- Exposes models for inference to **client** pipelines via standardized APIs\n",
    "    - Pipeline code stays the same even as the model changes or the service moves\n",
    "    - Centralized model repositories keep all clients on the same page\n",
    "- Containerization makes deployments portable to meet workload demands\n",
    "    - Minimizes environment management overhead\n",
    "    - Integration with container management servicse like Kubernetes leads to easy scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9154848",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deployment scenarios\n",
    "- IaaS represents a _software_ model for managing inference execution on heterogenous _hardware_\n",
    "- Not tied to any _particular_ hardware platform or deployment location\n",
    "    - Tune to meet the needs of each use case\n",
    "\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/triton-ldg.png\" height=\"auto\" width=\"350px\" class=\"left\" />\n",
    "    <img src=\"images/triton-cloud.png\" height=\"auto\" width=\"350px\" class=\"right\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff55bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Triton Inference Server](https://github.com/triton-inference-server/server)\n",
    "<div class=\"float100\">\n",
    "    <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/12/triton-1.png\" width=\"400px\" height=\"auto\" class=\"right padded\"/>\n",
    "        <p>Off-the-shelf inference service developed and maintained by NVIDIA.</p>\n",
    "</div>\n",
    "<div class=\"unpadded\">\n",
    "    <list>\n",
    "        <li class=\"explicit\">Efficient scheduling of GPU resources</li>\n",
    "        <li class=\"explicit\">Multiple framework backend support</li>\n",
    "        <li class=\"explicit\">Pre-built containers released monthly to simplify dependency management</li>\n",
    "        <li class=\"explicit\">Dynamic model versioning and ensemble scheduling</li>\n",
    "        <li class=\"explicit\">Separately installable <a href=\"https://github.com/triton-inference-server/clien\">client libraries</a>\n",
    "    </list>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf2bcf",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Latency, Throughput, Cost\n",
    "Fundamental trade-off in IaaS deployments\n",
    "- **latency** - time required to complete any single inference request\n",
    "- **throughput** - rate at which inference requests can be completed\n",
    "- **cost** - expense incurred for resources associated with inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e18eb",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div class=\"padded\">\n",
    "    <img src=\"images/metrics.png\" height=\"auto\" width=\"700px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e978b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Latency, Throughput, and Cost\n",
    "- Functions of deployment environment and configuration\n",
    "- Optimal value defined by constraints of use case\n",
    "- Measure this function in your environment to find optimal operating point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05268696",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div class=\"padded\">\n",
    "    <img src=\"images/latency-throughput-cost.png\" height=\"auto\" width=\"700px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40cbda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LIGO use cases\n",
    "## Results processing LIGO data with IaaS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9bef5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IaaS challenges with streaming time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadf5e5",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Inference happens on fixed length **snapshots** of the time series at one moment in time\n",
    "- Snapshots are sampled at some **inference sampling rate** $r \\leq f_s$\n",
    "    - Higher $r$ increases compute load $\\rightarrow$ shift in latency/throughput/cost surface\n",
    "    - Potential benefits to prediction quality, lower latency\n",
    "- Overlapping data creates enormous I/O load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd7034",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/snapshotter_overlap.png\" height=\"auto\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4e7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Challenges with streaming time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9bad3",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Introduce a model on server which maintains the most recent snapshot as a \"state\"\n",
    "    - Only need to stream state updates of _new_ data\n",
    "    - Improves I/O, but introduces a serial step which can limit parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a6b35",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div class=\"center\">\n",
    "    <img src=\"images/snapshotter_action.png\" height=\"auto\" width=\"180px\" class=\"unpadded\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486ee1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Offline Pipelines\n",
    "- For each server, assign $k$ clients\n",
    "    - With $n$ servers, each client is assigned $\\frac{1}{nk}$ of the total dataset to process\n",
    "    - Each client has an associated snapshotter state, must be routed to same server\n",
    "- Inference sampling rate has no bearing on data generation rate\n",
    "    - Determines the _number_ of frames needed to process for a given length of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ab098",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DeepClean Offline\n",
    "- Processed ~1 month of data during O3\n",
    "<div class=\"center padded\">\n",
    "    <img src=\"images/dc-offline.png\" height=\"auto\" width=\"1200px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b37fbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deepclean + BBHnet Offline\n",
    "Implemented using several different frameworks:\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/ensemble.png\" height=\"auto\" width=\"1000px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33019c6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deepclean + BBHnet Offline\n",
    "- Processed ~27 hours of data during O2\n",
    "- Strong scaling + elastic demand makes cloud ideal environment\n",
    "    - Cost nearly constant as a function of GPU usage\n",
    "    - Optimal scale $\\rightarrow\\infty$\n",
    "    - Trade-off shifts to prediction quality from higher $r$ vs. cost, need to quantify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca23c5",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/e2e-offline.png\" height=\"800px\" width=\"1200px\" class=\"padded\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaa202",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HEPCloud\n",
    "Framework from HEP community for sustained, high-throughput inference using Condor APIs\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/hepcloud.png\" height=\"auto\" width=\"800px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba346e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DeepClean Online\n",
    "- Overlapping outputs due to streaming time series\n",
    "    - \"Fully online\" inference scheme causes negative impacts on PSD\n",
    "    - Can trade off some latency for improved quality through aggregation\n",
    "    - Working on improving trade-off via better training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da6bbe",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/dc-output-overlap.png\" height=\"auto\" width=\"300px\" class=\"padded\" />\n",
    "<img src=\"images/psd-latency.png\" height=\"auto\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eed5f2",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DeepClean Online\n",
    "- Run on shared memory data replay frames\n",
    "- Inference sampling rate $r$ dictates _average_ data generation rate\n",
    "    - Frames become available in 1 second increments, true data generation rate is roughly a square wave\n",
    "- One data stream = one snapshotter\n",
    "    - Serial update causes queue build-up, no benefit to extra scale downstream\n",
    "    - Optimizing this step highest priority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4843e",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/dc-online.png\" height=\"auto\" width=\"1200px\" class=\"padded\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de7c21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next Steps\n",
    "## Taking IaaS into production at LIGO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ab947",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expanding Scope\n",
    "- IaaS model applicable to a wide range of applications\n",
    "- Building tools and infrasturcture for easy integration of newer and larger models\n",
    "    - <a href=\"https://github.com/fastmachinelearning/gw-iaas\">https://github.com/fastmachinelearning/gw-iaas</a>\n",
    "\n",
    "<div class=\"float100\">\n",
    "    <img src=\"images/a3d3.png\" height=\"auto\" width=\"400px\" class=\"left\" />\n",
    "    <img src=\"images/venn-diagram.png\" height=\"auto\" width=\"400px\" class=\"right\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a87842",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DeepClean Online Production Deployment\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/dc-production.png\" height=\"auto\" width=\"800px\" class=\"left\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc30b22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "scroll": true,
   "slideNumber": true,
   "theme": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
